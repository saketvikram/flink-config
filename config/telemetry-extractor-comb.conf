job {
  env = "local"
  # set to false so that flink does not connect to state backend
  enable.distributed.checkpointing = false
  statebackend {
    blob {
      storage {
        account = "blob.storage.account"
        container = "telemetry-container"
        checkpointing.dir = "flink-jobs"
      }
    }

    base.url = "wasbs://"${job.statebackend.blob.storage.container}"@"${job.statebackend.blob.storage.account}"/"${job.statebackend.blob.storage.checkpointing.dir}
  }
}

kafka {
  broker-servers = "10.0.0.5:9092"
  zookeeper = "10.0.0.5:2181"
  producer.broker-servers = "10.0.0.5:9092"
  consumer.broker-servers = "10.0.0.5:9092"
  producer.max-request-size = 5242880

  input.topic = ${job.env}".telemetry.ingest"
  output.success.topic = ${job.env}".telemetry.raw"
  output.log.route.topic = ${job.env}".druid.events.log"
  output.duplicate.topic = ${job.env}".telemetry.extractor.duplicate"
  output.failed.topic = ${job.env}".telemetry.failed"
  output.batch.failed.topic = ${job.env}".telemetry.extractor.failed"
  output.assess.raw.topic = ${job.env}".telemetry.assess.raw"
  event.max.size = "1048576" # Max is only 1MB
  groupId = ${job.env}"-telemetry-extractor-group"

}



task {
  parallelism = 1
  consumer.parallelism = 1
  checkpointing.interval = 60000
  restart-strategy.attempts = 3
  restart-strategy.delay = 30000 # in milli-seconds
  downstream.operators.parallelism = 1
}

redisdb.connection.timeout = 30000

redis {
  host = 10.0.0.6
  port = 6379
  database {
    duplicationstore.id = 1
    key.expiry.seconds = 3600
  }
}

redis-meta {
  host = 10.0.0.6
  port = 6379
  database {
    contentstore.id = 5
  }

}

# we'll see about these later
postgres {
  host = localhost
  port = 5430
  maxConnections = 2
  user = "postgres"
  password = "postgres"
}

lms-cassandra {
  host = "localhost"
  port = "9042"
}

redact.events.list = ["ASSESS","RESPONSE"]
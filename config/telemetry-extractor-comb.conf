job {
  env = "dp"
  # set to false so that flink does not connect to state backend
  enable.distributed.checkpointing = false
  statebackend {
    blob {
      storage {
        account = "blob.storage.account"
        container = "telemetry-container"
        checkpointing.dir = "flink-jobs"
      }
    }

    base.url = "wasbs://"${job.statebackend.blob.storage.container}"@"${job.statebackend.blob.storage.account}"/"${job.statebackend.blob.storage.checkpointing.dir}
  }
}

kafka {
  broker-servers = "10.0.0.5:9092"
  producer.broker-servers = "10.0.0.5:9092"
  consumer.broker-servers = "10.0.0.5:9092"
  zookeeper = "10.0.0.5:2181"
  producer.batch.size = 98304

  producer {
    max-request-size = 5242880
    linger.ms = 10
  }

  input.topic = "dp.telemetry.ingest"
  output.success.topic = "dp.telemetry.raw"
  output.log.route.topic = "dp.druid.events.log"
  output.duplicate.topic = "dp.telemetry.extractor.duplicate"
  output.failed.topic = "dp.telemetry.failed"
  output.batch.failed.topic = "local.telemetry.extractor.failed"
  output.assess.raw.topic = "dp.telemetry.assess.raw"
  event.max.size = "1048576" # Max is only 1MB
  groupId = "local-telemetry-extractor-group"
}

task {
  parallelism = 1
  consumer.parallelism = 1
  checkpointing.interval = 60000
  checkpointing.compressed = true
  checkpointing.pause.between.seconds = 30000
  restart-strategy.attempts = 3
  restart-strategy.delay = 30000 # in milli-seconds
  downstream.operators.parallelism = 1
}

redisdb.connection.timeout = 30000

redis {
  host = 10.0.0.6
  port = 6379
  database {
    duplicationstore.id = 1
    key.expiry.seconds = 3600
  }
}

redis-meta {
  host = 10.0.0.6
  port = 6379
  database {
    contentstore.id = 5
  }

}

# we'll see about these later
postgres {
  host = 10.0.0.7
  port = 5432
  maxConnections = 2
  user = "analytics"
  password = "password"
}

lms-cassandra {
  host = "10.0.0.7"
  port = "9042"
}

redact.events.list = ["ASSESS","RESPONSE"]